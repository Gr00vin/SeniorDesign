{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "add13e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from tqdm.auto import tqdm\n",
    "import kagglehub\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bef4791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.2).\n"
     ]
    }
   ],
   "source": [
    "#get the data\n",
    "path = kagglehub.dataset_download(\"frankossai/apple-stock-aapl-historical-financial-news-data\")\n",
    "csvs = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "df_mod = pd.read_csv(os.path.join(path, csvs[0])).set_index('date')\n",
    "df = df_mod['content'].to_frame()[:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1f2725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the new data\n",
    "\n",
    "# Read the CSV\n",
    "df = pd.read_csv('data_gold.csv')\n",
    "\n",
    "# Step 1: Filter only rows where headline starts with \"PRECIOUS\"\n",
    "df = df[df['headline'].str.startswith('PRECIOUS', na=False)].copy()\n",
    "\n",
    "# Step 2: Remove \"PRECIOUS-\" from the headline\n",
    "df['headline'] = df['headline'].str.replace(r'^PRECIOUS-', '', regex=True)\n",
    "\n",
    "# Step 3: Function to clean the body text\n",
    "def clean_body(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    # First, normalize whitespace (tabs, newlines, etc.) to spaces for processing\n",
    "    # But we want to keep certain newlines, so let's be more careful\n",
    "    \n",
    "    # The synopsis pattern is:\n",
    "    # - Starts with bullet points (* followed by text)\n",
    "    # - May have \"(Updates with...)\" or similar\n",
    "    # - Has \"By Author Name\"\n",
    "    # - Then \"Date (Reuters) -\" which marks the start of the actual article\n",
    "    \n",
    "    # Pattern to match everything before \"(Reuters) -\" and remove it\n",
    "    # The article starts after \"Date (Reuters) - \"\n",
    "    pattern = r'^.*?\\b[A-Z][a-z]{2}\\s+\\d{1,2}\\s+\\(Reuters\\)\\s*-\\s*'\n",
    "    \n",
    "    cleaned = re.sub(pattern, '', text, flags=re.DOTALL)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Step 4: Function to flatten whitespace (replace all whitespace with single spaces)\n",
    "def flatten_whitespace(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    # Replace all whitespace (newlines, tabs, multiple spaces) with single space\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# Apply cleaning to body\n",
    "df['body'] = df['body'].apply(clean_body)\n",
    "\n",
    "# Flatten whitespace in all text columns\n",
    "df['headline'] = df['headline'].apply(flatten_whitespace)\n",
    "df['body'] = df['body'].apply(flatten_whitespace)\n",
    "\n",
    "# Convert first_created to datetime to extract year (handle mixed formats)\n",
    "df['first_created'] = pd.to_datetime(df['first_created'], format='ISO8601', utc=True)\n",
    "df['year'] = df['first_created'].dt.year\n",
    "\n",
    "# Save the cleaned CSV\n",
    "output_path = 'data_gold_cleaned.csv'\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a934c5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing year 2010 (1691 articles)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d44f170bf84705bd97121ae695497d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring 2010 articles with FinBERT:   0%|          | 0/1691 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m article_body \u001b[38;5;129;01min\u001b[39;00m tqdm(year_df[\u001b[33m'\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m'\u001b[39m], desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m articles with FinBERT\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m         sentiment, scores = \u001b[43manalyze_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m     44\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mError scoring text:\u001b[39m\u001b[33m\"\u001b[39m, ex)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36manalyze_sentiment\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Tokenize once; rely on truncation to fit model max length (512 tokens)\u001b[39;00m\n\u001b[32m     14\u001b[39m     tokenized = tokenizer(text, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, max_length=\u001b[32m512\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokenized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     probs = softmax(output.logits, dim=\u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m].tolist()\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(probs) != \u001b[32m3\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1695\u001b[39m, in \u001b[36mBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1687\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1688\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1689\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1690\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1691\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1692\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1693\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1695\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1696\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1701\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1702\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1703\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1707\u001b[39m pooled_output = outputs[\u001b[32m1\u001b[39m]\n\u001b[32m   1709\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.dropout(pooled_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1141\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1134\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1135\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1139\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1141\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1153\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1154\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:694\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    683\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    684\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    685\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    691\u001b[39m         output_attentions,\n\u001b[32m    692\u001b[39m     )\n\u001b[32m    693\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m694\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    704\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:584\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    573\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    574\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    581\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m    582\u001b[39m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[32m    583\u001b[39m     self_attn_past_key_value = past_key_value[:\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    593\u001b[39m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:514\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    505\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    506\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    512\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    513\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    524\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:439\u001b[39m, in \u001b[36mBertSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[32m    432\u001b[39m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[32m    435\u001b[39m is_causal = (\n\u001b[32m    436\u001b[39m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    437\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    449\u001b[39m attn_output = attn_output.reshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m.all_head_size)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"ProsusAI/finbert\", clean_up_tokenization_spaces=True)\n",
    "model = BertForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "df = pd.read_csv(\"data_gold_cleaned.csv\")\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyze sentiment in a single pass without chunking.\"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"neutral\", [0.0, 0.0, 1.0]\n",
    "\n",
    "    text = text.strip()\n",
    "    try:\n",
    "        # Tokenize once; rely on truncation to fit model max length (512 tokens)\n",
    "        tokenized = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        output = model(**tokenized)\n",
    "        probs = softmax(output.logits, dim=1)[0].tolist()\n",
    "\n",
    "        if len(probs) != 3:\n",
    "            raise RuntimeError(\"Unexpected output shape\")\n",
    "\n",
    "        labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "        sentiment = labels[probs.index(max(probs))]\n",
    "        return sentiment, probs\n",
    "    except Exception as ex:\n",
    "        print(\"Error in analyze_sentiment:\", ex)\n",
    "        return \"neutral\", [0.0, 0.0, 1.0]\n",
    "\n",
    "\n",
    "\n",
    "# Process and save by year to avoid losing progress\n",
    "all_results = []\n",
    "\n",
    "for year in sorted(df['year'].unique()):\n",
    "    year_df = df[df['year'] == year].copy()\n",
    "    print(f\"\\nProcessing year {year} ({len(year_df)} articles)\")\n",
    "\n",
    "    year_sentiments = []\n",
    "    year_probabilities = []\n",
    "\n",
    "    for article_body in tqdm(year_df['body'], desc=f\"Scoring {year} articles with FinBERT\"):\n",
    "        try:\n",
    "            sentiment, scores = analyze_sentiment(article_body)\n",
    "        except Exception as ex:\n",
    "            print(\"Error scoring text:\", ex)\n",
    "            sentiment, scores = \"neutral\", [0.0, 0.0, 1.0]\n",
    "\n",
    "        year_sentiments.append(sentiment)\n",
    "        year_probabilities.append(scores)\n",
    "\n",
    "    year_df['sentiment'] = year_sentiments\n",
    "    year_df['probs'] = year_probabilities\n",
    "    year_df['sent_score'] = year_df['probs'].apply(lambda p: p[0] - p[1])\n",
    "\n",
    "    # Save this year's results immediately\n",
    "    year_output_file = f\"sentiment_data_cleaned/data_gold_finbert_{year}.csv\"\n",
    "    year_df.to_csv(year_output_file, index=False)\n",
    "    print(f\"Saved {year} results to {year_output_file}\")\n",
    "\n",
    "    all_results.append(year_df)\n",
    "\n",
    "# Combine all years and save final complete file\n",
    "df_complete = pd.concat(all_results, ignore_index=True)\n",
    "df_complete.to_csv(\"sentiment_data_cleaned/data_gold_finbert_complete.csv\", index=False)\n",
    "print(f\"\\nâœ“ All years processed and saved to data_gold_finbert_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9885376b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            first_created  daily_sentiment\n",
      "0  2025-01-02 22:24:34-05         0.864984\n",
      "1  2025-01-03 04:58:45-05         0.452268\n",
      "2  2025-01-03 10:15:01-05        -0.138539\n",
      "3  2025-01-07 10:06:25-05         0.815281\n",
      "4  2025-01-08 10:15:31-05        -0.440093\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"lseg_body_finbert_output.csv\")\n",
    "daily_summary = (df.groupby(\"first_created\")['sent_score'].mean().reset_index().rename(columns={'sent_score': 'daily_sentiment'}))\n",
    "\n",
    "print(daily_summary.head())\n",
    "\n",
    "\n",
    "df.to_csv(\"lseg_body_finbert_output.csv\", index=False)\n",
    "daily_summary.to_csv(\"lseg_body_daily_sentiment.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fde5809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>first_created</th>\n",
       "      <th>headline</th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>probs</th>\n",
       "      <th>sent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>51335158</td>\n",
       "      <td>2025-01-27 20:20:16-05</td>\n",
       "      <td>Gold holds ground as traders brace for Fed rat...</td>\n",
       "      <td>Gold prices firmed on Tuesday as focus shifted...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.010495555587112904, 0.9727091789245605, 0.0...</td>\n",
       "      <td>-0.962214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>51339087</td>\n",
       "      <td>2025-01-28 04:52:59-05</td>\n",
       "      <td>Gold stabilises after selloff as wider markets...</td>\n",
       "      <td>Gold prices held steady on Tuesday, anchored b...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.013177186250686646, 0.969575047492981, 0.01...</td>\n",
       "      <td>-0.956398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>51332659</td>\n",
       "      <td>2025-01-27 11:13:10-05</td>\n",
       "      <td>Gold retreats as investors liquidate positions...</td>\n",
       "      <td>Gold prices declined over 1% on Monday, retrea...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.016720503568649292, 0.9661612510681152, 0.0...</td>\n",
       "      <td>-0.949441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>51333141</td>\n",
       "      <td>2025-01-27 11:13:10-05</td>\n",
       "      <td>Gold retreats as investors liquidate positions...</td>\n",
       "      <td>Gold prices declined more than 1% on Monday, r...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.01757621020078659, 0.9663547873497009, 0.01...</td>\n",
       "      <td>-0.948779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>51345918</td>\n",
       "      <td>2025-01-28 22:45:30-05</td>\n",
       "      <td>Gold holds steady as investors eye Fed decisio...</td>\n",
       "      <td>Gold was little changed on Wednesday as market...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.016708968207240105, 0.9617332816123962, 0.0...</td>\n",
       "      <td>-0.945024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     item_id           first_created  \\\n",
       "55  51335158  2025-01-27 20:20:16-05   \n",
       "56  51339087  2025-01-28 04:52:59-05   \n",
       "53  51332659  2025-01-27 11:13:10-05   \n",
       "54  51333141  2025-01-27 11:13:10-05   \n",
       "61  51345918  2025-01-28 22:45:30-05   \n",
       "\n",
       "                                             headline  \\\n",
       "55  Gold holds ground as traders brace for Fed rat...   \n",
       "56  Gold stabilises after selloff as wider markets...   \n",
       "53  Gold retreats as investors liquidate positions...   \n",
       "54  Gold retreats as investors liquidate positions...   \n",
       "61  Gold holds steady as investors eye Fed decisio...   \n",
       "\n",
       "                                                 body sentiment  \\\n",
       "55  Gold prices firmed on Tuesday as focus shifted...  negative   \n",
       "56  Gold prices held steady on Tuesday, anchored b...  negative   \n",
       "53  Gold prices declined over 1% on Monday, retrea...  negative   \n",
       "54  Gold prices declined more than 1% on Monday, r...  negative   \n",
       "61  Gold was little changed on Wednesday as market...  negative   \n",
       "\n",
       "                                                probs  sent_score  \n",
       "55  [0.010495555587112904, 0.9727091789245605, 0.0...   -0.962214  \n",
       "56  [0.013177186250686646, 0.969575047492981, 0.01...   -0.956398  \n",
       "53  [0.016720503568649292, 0.9661612510681152, 0.0...   -0.949441  \n",
       "54  [0.01757621020078659, 0.9663547873497009, 0.01...   -0.948779  \n",
       "61  [0.016708968207240105, 0.9617332816123962, 0.0...   -0.945024  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"lseg_body_finbert_output.csv\")\n",
    "df.sort_values(by='sent_score').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfd7cfdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>first_created</th>\n",
       "      <th>headline</th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>probs</th>\n",
       "      <th>sent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>51252194</td>\n",
       "      <td>2025-01-13 21:56:55-05</td>\n",
       "      <td>Trump policy uncertainty lifts gold; US inflat...</td>\n",
       "      <td>Gold prices gained on Tuesday, buoyed by uncer...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[0.9136364459991455, 0.06271181255578995, 0.02...</td>\n",
       "      <td>0.850925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51197718</td>\n",
       "      <td>2025-01-02 22:24:34-05</td>\n",
       "      <td>Gold set for weekly rise; market awaits Trump'...</td>\n",
       "      <td>Gold edged up on Friday on a softer U.S. dolla...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[0.9196024537086487, 0.054618239402770996, 0.0...</td>\n",
       "      <td>0.864984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>51250795</td>\n",
       "      <td>2025-01-13 21:56:55-05</td>\n",
       "      <td>Trump policy uncertainty lifts gold; US inflat...</td>\n",
       "      <td>Gold prices gained on Tuesday, buoyed by uncer...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[0.9313398599624634, 0.04638082906603813, 0.02...</td>\n",
       "      <td>0.884959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>51358621</td>\n",
       "      <td>2025-01-30 05:38:45-05</td>\n",
       "      <td>Safe-haven gold rises on Trump tariff worries</td>\n",
       "      <td>Gold prices rose on Thursday as investors worr...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[0.9375680685043335, 0.03505799546837807, 0.02...</td>\n",
       "      <td>0.902510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>51356947</td>\n",
       "      <td>2025-01-30 05:38:45-05</td>\n",
       "      <td>Safe-haven gold rises amid Trump tariff worries</td>\n",
       "      <td>Safe-haven gold prices rose on Thursday as inv...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[0.9371510744094849, 0.03357581049203873, 0.02...</td>\n",
       "      <td>0.903575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     item_id           first_created  \\\n",
       "18  51252194  2025-01-13 21:56:55-05   \n",
       "0   51197718  2025-01-02 22:24:34-05   \n",
       "17  51250795  2025-01-13 21:56:55-05   \n",
       "66  51358621  2025-01-30 05:38:45-05   \n",
       "65  51356947  2025-01-30 05:38:45-05   \n",
       "\n",
       "                                             headline  \\\n",
       "18  Trump policy uncertainty lifts gold; US inflat...   \n",
       "0   Gold set for weekly rise; market awaits Trump'...   \n",
       "17  Trump policy uncertainty lifts gold; US inflat...   \n",
       "66      Safe-haven gold rises on Trump tariff worries   \n",
       "65    Safe-haven gold rises amid Trump tariff worries   \n",
       "\n",
       "                                                 body sentiment  \\\n",
       "18  Gold prices gained on Tuesday, buoyed by uncer...  positive   \n",
       "0   Gold edged up on Friday on a softer U.S. dolla...  positive   \n",
       "17  Gold prices gained on Tuesday, buoyed by uncer...  positive   \n",
       "66  Gold prices rose on Thursday as investors worr...  positive   \n",
       "65  Safe-haven gold prices rose on Thursday as inv...  positive   \n",
       "\n",
       "                                                probs  sent_score  \n",
       "18  [0.9136364459991455, 0.06271181255578995, 0.02...    0.850925  \n",
       "0   [0.9196024537086487, 0.054618239402770996, 0.0...    0.864984  \n",
       "17  [0.9313398599624634, 0.04638082906603813, 0.02...    0.884959  \n",
       "66  [0.9375680685043335, 0.03505799546837807, 0.02...    0.902510  \n",
       "65  [0.9371510744094849, 0.03357581049203873, 0.02...    0.903575  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by='sent_score').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27a1d670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_mod columns: ['title', 'content', 'link', 'symbols', 'tags', 'sentiment_polarity', 'sentiment_neg', 'sentiment_neu', 'sentiment_pos']\n",
      "\n",
      "df_mod shape: (29752, 9)\n",
      "\n",
      "df_mod sample with all columns:\n",
      "                                                                       title  \\\n",
      "date                                                                           \n",
      "2024-11-27T16:39:00+00:00  Berkshire Stock Hits Record Even as Company Re...   \n",
      "2024-11-26T00:00:00+00:00                      What Is a Stock Market Index?   \n",
      "2024-11-26T00:00:00+00:00  Could Investing $1,000 in Apple Make You a Mil...   \n",
      "2024-11-26T00:00:00+00:00                       Dow Jones Industrial Average   \n",
      "2024-11-26T00:00:00+00:00                         What Is the S&P 500 Index?   \n",
      "\n",
      "                                                                     content  \\\n",
      "date                                                                           \n",
      "2024-11-27T16:39:00+00:00  Warren Buffettâ€™s caution, his advancing age, a...   \n",
      "2024-11-26T00:00:00+00:00                      What Is a Stock Market Index?   \n",
      "2024-11-26T00:00:00+00:00  Could Investing $1,000 in Apple Make You a Mil...   \n",
      "2024-11-26T00:00:00+00:00                       Dow Jones Industrial Average   \n",
      "2024-11-26T00:00:00+00:00                         What Is the S&P 500 Index?   \n",
      "\n",
      "                                                                        link  \\\n",
      "date                                                                           \n",
      "2024-11-27T16:39:00+00:00  https://finance.yahoo.com/m/f5df3aa4-364b-31d6...   \n",
      "2024-11-26T00:00:00+00:00  https://www.fool.com/investing/stock-market/in...   \n",
      "2024-11-26T00:00:00+00:00  https://www.fool.com/investing/2024/11/26/coul...   \n",
      "2024-11-26T00:00:00+00:00  https://www.fool.com/investing/stock-market/in...   \n",
      "2024-11-26T00:00:00+00:00  https://www.fool.com/investing/stock-market/in...   \n",
      "\n",
      "                                                                     symbols  \\\n",
      "date                                                                           \n",
      "2024-11-27T16:39:00+00:00  0R2V.IL, AAPL.BA, AAPL.MX, AAPL.NEO, AAPL.SN, ...   \n",
      "2024-11-26T00:00:00+00:00                          AAPL.US, AMZN.US, MSFT.US   \n",
      "2024-11-26T00:00:00+00:00                                            AAPL.US   \n",
      "2024-11-26T00:00:00+00:00  AAPL.US, AMGN.US, AMZN.US, CSCO.US, GOOG.US, G...   \n",
      "2024-11-26T00:00:00+00:00  AAPL.US, AMZN.US, GOOG.US, GOOGL.US, META.US, ...   \n",
      "\n",
      "                          tags  sentiment_polarity  sentiment_neg  \\\n",
      "date                                                                \n",
      "2024-11-27T16:39:00+00:00  NaN                 0.0            0.0   \n",
      "2024-11-26T00:00:00+00:00  NaN                 0.0            0.0   \n",
      "2024-11-26T00:00:00+00:00  NaN                 0.0            0.0   \n",
      "2024-11-26T00:00:00+00:00  NaN                 0.0            0.0   \n",
      "2024-11-26T00:00:00+00:00  NaN                 0.0            0.0   \n",
      "\n",
      "                           sentiment_neu  sentiment_pos  \n",
      "date                                                     \n",
      "2024-11-27T16:39:00+00:00            1.0            0.0  \n",
      "2024-11-26T00:00:00+00:00            1.0            0.0  \n",
      "2024-11-26T00:00:00+00:00            1.0            0.0  \n",
      "2024-11-26T00:00:00+00:00            1.0            0.0  \n",
      "2024-11-26T00:00:00+00:00            1.0            0.0  \n"
     ]
    }
   ],
   "source": [
    "# Check what columns df_mod has and compare original sentiment values\n",
    "print(\"df_mod columns:\", df_mod.columns.tolist())\n",
    "print(\"\\ndf_mod shape:\", df_mod.shape)\n",
    "print(\"\\ndf_mod sample with all columns:\")\n",
    "print(df_mod.head())\n",
    "\n",
    "if 'sentiment' in df_mod.columns:\n",
    "    print(\"\\n--- Comparison: Original sentiment vs FinBERT output ---\")\n",
    "    comparison = df.copy()\n",
    "    comparison['original_sentiment'] = df_mod.loc[df.index, 'sentiment']\n",
    "    print(comparison[['content', 'original_sentiment', 'sentiment', 'sent_score']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5992faf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df columns: ['content']\n",
      "df shape: (80, 1)\n",
      "\n",
      "df.head():\n",
      "                                                                     content\n",
      "date                                                                        \n",
      "2024-11-27T16:39:00+00:00  Warren Buffettâ€™s caution, his advancing age, a...\n",
      "2024-11-26T00:00:00+00:00                      What Is a Stock Market Index?\n",
      "2024-11-26T00:00:00+00:00  Could Investing $1,000 in Apple Make You a Mil...\n",
      "2024-11-26T00:00:00+00:00                       Dow Jones Industrial Average\n",
      "2024-11-26T00:00:00+00:00                         What Is the S&P 500 Index?\n",
      "\n",
      "âš ï¸  WARNING: df doesn't have sentiment columns. Rerun cell 3 to compute sentiment scores.\n"
     ]
    }
   ],
   "source": [
    "# First, check what columns df actually has\n",
    "print(\"df columns:\", df.columns.tolist())\n",
    "print(\"df shape:\", df.shape)\n",
    "print(\"\\ndf.head():\")\n",
    "print(df.head())\n",
    "\n",
    "if 'sentiment' not in df.columns:\n",
    "    print(\"\\nâš ï¸  WARNING: df doesn't have sentiment columns. Rerun cell 3 to compute sentiment scores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978fad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the Kaggle dataset's sentiment values look like for the first 80 rows\n",
    "print(\"=== Kaggle Dataset Sentiment Distribution (first 80 rows) ===\\n\")\n",
    "kaggle_80 = df_mod.iloc[:80]\n",
    "\n",
    "print(f\"sentiment_polarity stats:\")\n",
    "print(f\"  Mean: {kaggle_80['sentiment_polarity'].mean():.4f}\")\n",
    "print(f\"  Min: {kaggle_80['sentiment_polarity'].min():.4f}\")\n",
    "print(f\"  Max: {kaggle_80['sentiment_polarity'].max():.4f}\")\n",
    "print(f\"  Median: {kaggle_80['sentiment_polarity'].median():.4f}\")\n",
    "\n",
    "print(f\"\\nsentiment_pos stats:\")\n",
    "print(f\"  Mean: {kaggle_80['sentiment_pos'].mean():.4f}\")\n",
    "print(f\"  Max: {kaggle_80['sentiment_pos'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nsentiment_neg stats:\")\n",
    "print(f\"  Mean: {kaggle_80['sentiment_neg'].mean():.4f}\")\n",
    "print(f\"  Max: {kaggle_80['sentiment_neg'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nsentiment_neu stats:\")\n",
    "print(f\"  Mean: {kaggle_80['sentiment_neu'].mean():.4f}\")\n",
    "print(f\"  Min: {kaggle_80['sentiment_neu'].min():.4f}\")\n",
    "\n",
    "# Determine label by max score\n",
    "kaggle_80_copy = kaggle_80.copy()\n",
    "kaggle_80_copy['label'] = kaggle_80[['sentiment_pos', 'sentiment_neg', 'sentiment_neu']].idxmax(axis=1).str.replace('sentiment_', '')\n",
    "print(\"\\nKaggle sentiment label distribution:\")\n",
    "print(kaggle_80_copy['label'].value_counts())\n",
    "\n",
    "print(\"\\n=== Sample articles with their Kaggle sentiment ===\")\n",
    "print(kaggle_80_copy[['title', 'sentiment_polarity', 'sentiment_pos', 'sentiment_neg', 'sentiment_neu', 'label']].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
